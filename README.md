# Clear Street Data Assessment Solution
The goal of this repository was to explore various calibration models and compare their performances. However, the bulk of my time was spent working on the feature selection method, which was not to great satisfaction. Regardless, I implemented three feature selection techniques and tested them using a Linear Regression model to inconclusive results. My lack of finance domain knowledge certainly made it difficult for me to look at the data in much of a meaningful way. I found myself chasing after features that exhibited time-series behavior, but that was not a fruitful endeavor. My time would have been better served seeking other approaches that I had aimed to. I had set up a XGBoosted trees module as well as a Forwardfeeding Neural Network module, but I did not have much time to explore those options. Nonetheless, they are included in the repository, with a Neural Network training script that was too computationally intensive to complete within the allotted time frame. Given more time and computational power, I would have liked to explore this project more, but I have to submit my progress thus far.

## Data Acuqisition and Cleaning
I acquired the data as mentioned in the attached PDF file and cleaned it using the given guidelines. More specifically, I checked for '999999' values, replaced them with 'NaN' values, and dropped rows where more than half the entries were 'NaN'. I also implemented a function to check for the 'Q1' and 'Q2' values to prepare data for training specifically for the 'Y1' and 'Y2' calibrations, respectively. These functions can be found in [cleaning.py](cleaning.py).

## Feature Selection

I used three methods to test for feature selection: LASSO, Ridge, and Elastic Net. Although they are part of the same family (L1, L2 regularization), they gave me varying results. The results are documented in [feature_selection_summary.txt](feature_selection_summary.txt). I used the first 70% of data files (from 20220103-20221110) as training data and the remaining as testing data for the Linear Regression in the next section.

I tried various alphas for the regularization methods, and found that around alpha of a 100, LASSO would select the same features for 'Y1' and 'Y2'. I did some research and found a method for determining the optimal alpha, implemented in [hyperparam_search.py](hyperparam_search.py). However, I found that below an alpha of 1, the regularization methods did not converge, and hence stuck to an alpha of 1. Perhaps the methods would have worked better with an increased number of iterations for convergence and a lower alpha.

I also attempted to include an autocorrelation factor in the coefficients of the selected features. This was an attempt to fish out features that exhibited time-series profiles. However, I was unable to get any meaningful results from this approach. I considered doing time-series analysis on the 'Y1' and 'Y2' data to determine seasonality and prominent lag values that I may have been able to use to find important autocorrelations between some of the features, but I did not have the time. Perhaps this is another fruitful approach that might be able to isolate features that have strong time-series-based correlations to the 'Y' data.

I ultimately chose an alpha of 1 for all my regularization methods, and an even L1/L2 split for the Elastic Net regime. I then used the selected features to build Linear Regression models.

## Linear Regression

I used the standard scikit-learn Linear Regression model. I did not modify the model in any way and let it run on the aforementioned training sample for each of the features identified by the regularization methods listed above. I then tested these models on the remaining data, and the results were inconclusive, to say the least. I initially used the MAPE (mean absolute percentage error) to compare the predicted data to the actual data, and this gave me extremely large errors due to the 'Y1' values being very close to zero at times. The larger values completely skewed the average error, so I had to resort to using RMSE (root mean squared error) instead. The values from RMSE were significantly larger than the standard deviation of the data, indicating that the Linear Regressive model was not making successful predictions. I suspect this is due to poor feature selection from the regularization methods. The results are summarized in [Linear_Regression_Summary.py](Linear_Regression_Summary.txt).

I had previously considered using SARIMAX in order to address the time-series aspect of the data. However, due to the stepwise predictive nature of the model as well as computational complexity, I decided to forego the approach. The accompanying PDF document had suggested treating the time variable as windows to different regressive fits, but I was unable to determine an informed time division in order to try something of the nature. I tried studying the 'Y1' and 'Y2' data for any noticeable trends or dependencies on the time of day, but was unable to find anything that suggested significant differences based on some consistent time period. However, I think this may be a good place to search further, as it would potentially aid in better feature selection as well.

## Other Calibration Methods

Other methods I had considered were XGBoost and Neural Networks.

I lack experience with XGBoost, so I was planning on experimenting it with it if time allowed. I did spend a good deal of time on building a Neural Network, however. I initially considered using a LSTM to account for the time-series aspect. However, since we are dealing with a multivariate time-series, that too with 375 potential exogenous variables, I quickly realized that this was likely a dead end. I was planning on using a hybrid approach whereupon the feature selection methods would give me a small selection of significant features that showed promise in Linear Regression. Then I wanted to create a LSTM that took certain time windows and these selected variables as features for a potentially robust deep learning calibration. However, this came to no avail as the features selected by my regularization methods were not promising. I then shifted my focus to a more traditional forward feeding neural network. I was able to complete the model and start its training, but I lacked the computational power to train it on a sufficiently significant sample of the training data to a reasonable accuracy. However, given more time and computational power, perhaps there is promise there.

## Conclusion

All in all, although the results of my work were inconclusive, I enjoyed the opportunity to work on this project. While the lack of finance domain knowledge severely curtailed my ability to engineer the data, it was a worthwhile experience in studying a completely novel kind of data. I have included the coefficients from my Linear Regression models in [Linear_Model_Coefficients.txt](Linear_Model_Coefficients.txt), as tasked by the assignment. Although I am forced to conclude that my findings were absolutely inconclusive and my predictive model did not do a very good job, I am sure I can learn more and improve upon it. I would appreciate any continued consideration, and I maintain a strong enthusiasm about working at Clear Street and learning more about the world of quantitative finance.


